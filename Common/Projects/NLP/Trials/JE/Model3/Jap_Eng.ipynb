{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Saved: pkl/english-japanese_3.pkl\n",
      "[go] => [行けx動]\n",
      "[go] => [行きx動 なさいx動]\n",
      "[hi] => [こんにちはx感]\n",
      "[hi] => [もしもしx感]\n",
      "[hi] => [やっx動 ほx動 ーx名]\n",
      "[hi] => [こんにちはx感]\n",
      "[run] => [走れx動]\n",
      "[run] => [走っx動 てx助]\n",
      "[who] => [誰x名]\n",
      "[wow] => [すごいx形]\n",
      "[wow] => [ワォx名]\n",
      "[wow] => [わx助 ぉx名]\n",
      "[wow] => [おx接 ーx名]\n",
      "[fire] => [火事x名 だx助]\n",
      "[fire] => [火事x名]\n",
      "[fire] => [撃てx動]\n",
      "[help] => [助けx動 てx助]\n",
      "[help] => [助けx動 てx助 くれx動]\n",
      "[jump] => [飛び越えろx動]\n",
      "[jump] => [跳べx動]\n",
      "[jump] => [飛び降りろx動]\n",
      "[jump] => [飛び跳ねx動 てx助]\n",
      "[jump] => [ジャンプx名 しx動 てx助]\n",
      "[jump] => [跳べx動]\n",
      "[jump] => [飛び跳ねx動 てx助]\n",
      "[jump] => [ジャンプx名 しx動 てx助]\n",
      "[stop] => [やめろx動]\n",
      "[stop] => [止まれx動]\n",
      "[wait] => [待っx動 てx助]\n",
      "[go on] => [続けx動 てx助]\n",
      "[go on] => [進んx動 でx助]\n",
      "[go on] => [進めx動]\n",
      "[go on] => [続けろx動]\n",
      "[hello] => [こんにちはx感]\n",
      "[hello] => [もしもしx感]\n",
      "[hello] => [こんにちはx感]\n",
      "[hurry] => [急げx動]\n",
      "[i see] => [なるほどx感]\n",
      "[i see] => [なるほどx感 ねx助]\n",
      "[i see] => [わかっx動 たx助]\n",
      "[i see] => [わかりx動 ましx助 たx助]\n",
      "[i see] => [そうx副 ですx助 かx助]\n",
      "[i see] => [そうx副 なx助 んx名 だx助]\n",
      "[i see] => [そっx名 かx助]\n",
      "[i try] => [頑張っx動 てx助 みるx動]\n",
      "[i try] => [やっx動 てx助 みるx動]\n",
      "[i try] => [試しx動 てx助 みるx動]\n",
      "[i try] => [やっx動 てx助 みよx動 うx助]\n",
      "[i try] => [トライx名 しx動 てx助 みるx動]\n",
      "[i won] => [俺x名 のx助 勝ちx名 ーx名]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('%%') for line in  lines]\n",
    "    return pairs\n",
    " \n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "filename = 'data/jap_dataset_2.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-jap pairs\n",
    "pairs = to_pairs(doc)\n",
    "print(type(pairs))\n",
    "\n",
    "clean_pairs = array(pairs)\n",
    "# clean sentences\n",
    "# clean_pairs = clean_pairs(pairs)\n",
    "# print(clean_pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'pkl/english-japanese_3.pkl')\n",
    "# spot check\n",
    "for i in range(50):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: pkl/english-japanese_3-both.pkl\n",
      "Saved: pkl/english-japanese_3-train.pkl\n",
      "Saved: pkl/english-japanese_3-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('pkl/english-japanese_3.pkl')\n",
    "# print(raw_dataset)\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# print(dataset)\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# print(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "# save\n",
    "save_clean_data(dataset, 'pkl/english-japanese_3-both.pkl')\n",
    "save_clean_data(train, 'pkl/english-japanese_3-train.pkl')\n",
    "save_clean_data(test, 'pkl/english-japanese_3-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2716\n",
      "English Max Length: 7\n",
      "Japanese Vocabulary Size: 4147\n",
      "Japanese Max Length: 19\n",
      "Max Length: 19\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 19, 256)           1061632   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 19, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 19, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 19, 2716)          698012    \n",
      "=================================================================\n",
      "Total params: 2,810,268\n",
      "Trainable params: 2,810,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      " - 127s - loss: 1.8793 - acc: 0.8124 - val_loss: 1.2438 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.24382, saving model to models/jap_eng_model3.h5\n",
      "Epoch 2/20\n",
      " - 121s - loss: 1.1734 - acc: 0.8250 - val_loss: 1.1640 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.24382 to 1.16404, saving model to models/jap_eng_model3.h5\n",
      "Epoch 3/20\n",
      " - 120s - loss: 1.1264 - acc: 0.8276 - val_loss: 1.1484 - val_acc: 0.8288\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.16404 to 1.14841, saving model to models/jap_eng_model3.h5\n",
      "Epoch 4/20\n",
      " - 120s - loss: 1.1057 - acc: 0.8283 - val_loss: 1.1368 - val_acc: 0.8285\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.14841 to 1.13682, saving model to models/jap_eng_model3.h5\n",
      "Epoch 5/20\n",
      " - 105s - loss: 1.0866 - acc: 0.8291 - val_loss: 1.1282 - val_acc: 0.8289\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.13682 to 1.12820, saving model to models/jap_eng_model3.h5\n",
      "Epoch 6/20\n",
      " - 104s - loss: 1.0691 - acc: 0.8296 - val_loss: 1.1177 - val_acc: 0.8289\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.12820 to 1.11767, saving model to models/jap_eng_model3.h5\n",
      "Epoch 7/20\n",
      " - 104s - loss: 1.0503 - acc: 0.8307 - val_loss: 1.1087 - val_acc: 0.8306\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.11767 to 1.10873, saving model to models/jap_eng_model3.h5\n",
      "Epoch 8/20\n",
      " - 103s - loss: 1.0252 - acc: 0.8347 - val_loss: 1.0868 - val_acc: 0.8346\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.10873 to 1.08681, saving model to models/jap_eng_model3.h5\n",
      "Epoch 9/20\n",
      " - 104s - loss: 0.9926 - acc: 0.8391 - val_loss: 1.0748 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.08681 to 1.07481, saving model to models/jap_eng_model3.h5\n",
      "Epoch 10/20\n",
      " - 104s - loss: 0.9619 - acc: 0.8420 - val_loss: 1.0573 - val_acc: 0.8402\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.07481 to 1.05735, saving model to models/jap_eng_model3.h5\n",
      "Epoch 11/20\n",
      " - 105s - loss: 0.9324 - acc: 0.8444 - val_loss: 1.0368 - val_acc: 0.8422\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.05735 to 1.03684, saving model to models/jap_eng_model3.h5\n",
      "Epoch 12/20\n",
      " - 104s - loss: 0.9023 - acc: 0.8469 - val_loss: 1.0199 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.03684 to 1.01987, saving model to models/jap_eng_model3.h5\n",
      "Epoch 13/20\n",
      " - 104s - loss: 0.8677 - acc: 0.8499 - val_loss: 0.9968 - val_acc: 0.8458\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.01987 to 0.99678, saving model to models/jap_eng_model3.h5\n",
      "Epoch 14/20\n",
      " - 106s - loss: 0.8350 - acc: 0.8531 - val_loss: 0.9777 - val_acc: 0.8455\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.99678 to 0.97772, saving model to models/jap_eng_model3.h5\n",
      "Epoch 15/20\n",
      " - 110s - loss: 0.8037 - acc: 0.8556 - val_loss: 0.9764 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.97772 to 0.97641, saving model to models/jap_eng_model3.h5\n",
      "Epoch 16/20\n",
      " - 107s - loss: 0.7736 - acc: 0.8582 - val_loss: 0.9562 - val_acc: 0.8498\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.97641 to 0.95624, saving model to models/jap_eng_model3.h5\n",
      "Epoch 17/20\n",
      " - 105s - loss: 0.7436 - acc: 0.8609 - val_loss: 0.9443 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.95624 to 0.94427, saving model to models/jap_eng_model3.h5\n",
      "Epoch 18/20\n",
      " - 106s - loss: 0.7134 - acc: 0.8641 - val_loss: 0.9396 - val_acc: 0.8521\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.94427 to 0.93962, saving model to models/jap_eng_model3.h5\n",
      "Epoch 19/20\n",
      " - 107s - loss: 0.6857 - acc: 0.8670 - val_loss: 0.9249 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.93962 to 0.92493, saving model to models/jap_eng_model3.h5\n",
      "Epoch 20/20\n",
      " - 105s - loss: 0.6570 - acc: 0.8697 - val_loss: 0.9115 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.92493 to 0.91149, saving model to models/jap_eng_model3.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c10ce01d68>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pydot\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "#     y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('pkl/english-japanese_3-both.pkl')\n",
    "train = load_clean_sentences('pkl/english-japanese_3-train.pkl')\n",
    "test = load_clean_sentences('pkl/english-japanese_3-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "jap_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "jap_vocab_size = len(jap_tokenizer.word_index) + 1\n",
    "jap_length = max_length(dataset[:, 1])\n",
    "print('Japanese Vocabulary Size: %d' % jap_vocab_size)\n",
    "print('Japanese Max Length: %d' % (jap_length))\n",
    "\n",
    "max_len = max(jap_length,eng_length)\n",
    "print('Max Length: %d' % (max_len))\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(jap_tokenizer, max_len, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, max_len, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(jap_tokenizer, max_len, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, max_len, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "# define model\n",
    "model = define_model(jap_vocab_size, eng_vocab_size, max_len, max_len, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='japmodel_idk.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'models/jap_eng_model3.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=20, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min Length: 19\n",
      "train\n",
      "src=[どこx名 にx助 行きx動 ましょx助 うx助], target=[where shall we go], predicted=[lets i i it]\n",
      "src=[窓x名 をx助 閉めx動 てx助], target=[close the window], predicted=[close the room]\n",
      "src=[メアリーx名 はx助 本x名 のx助 虫x名 ですx助], target=[mary is a bookworm], predicted=[this is a]\n",
      "src=[お待ちx名 くださいx動], target=[hold on], predicted=[please me]\n",
      "src=[あのx連 鞄x名 がx助 欲しいx形], target=[i want that bag], predicted=[i want to]\n",
      "src=[座りx名 なx助 よx助], target=[have a seat], predicted=[please a a]\n",
      "src=[誰x名 もx助 死なx動 なかっx助 たx助], target=[nobody has died], predicted=[they did]\n",
      "src=[トムx名 はx助 賢かっx形 たx助], target=[tom was clever], predicted=[tom was]\n",
      "src=[誰x名 もx助 がx助 彼x名 をx助 愛しx動 てx助 いるx動], target=[everybody loves him], predicted=[tom loves her]\n",
      "src=[彼x名 はx助 ドアx名 をx助 あけx動 たx助], target=[he opened the door], predicted=[he was the]\n",
      "test\n",
      "src=[彼女x名 はx助 出x動 てx助 行っx動 てx助 しまっx動 たx助], target=[she has gone out], predicted=[she was a]\n",
      "src=[どうぞx副 、x記 お先にx副], target=[go ahead], predicted=[keep up]\n",
      "src=[それx名 をx助 見るx動 ことx名 がx助 できx動 ますx助 かx助], target=[can i see that], predicted=[what you you you]\n",
      "src=[カメラx名 はx助 おx接 持ちx動 ですx助 かx助], target=[do you have a camera], predicted=[do you like a]\n",
      "src=[おいx感 、x記 おまえx名 、x記 逃げるx動 なx助], target=[dont you go away], predicted=[dont go go]\n",
      "src=[説明x名 しx動 てx助 くださいx動], target=[please explain it], predicted=[please you a]\n",
      "src=[ああx感 、x記 お腹x名 がx助 空いx動 たx助], target=[im hungry], predicted=[im hungry hungry]\n",
      "src=[彼女x名 はx助 なんてx助 美しいx形 のx名 でしょx助 うx助], target=[how beautiful she is], predicted=[she is]\n",
      "src=[邪険x名 にx助 しx動 ないx助 でx助 下さいx動 よx助], target=[please dont be cold], predicted=[dont worry me]\n",
      "src=[変わりx動 はx助 ないx形 かいx助], target=[whats new], predicted=[is is is]\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('pkl/english-japanese_3-both.pkl')\n",
    "train = load_clean_sentences('pkl/english-japanese_3-train.pkl')\n",
    "test = load_clean_sentences('pkl/english-japanese_3-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "jap_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "jap_vocab_size = len(jap_tokenizer.word_index) + 1\n",
    "jap_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "\n",
    "max_len = max(jap_length,eng_length)\n",
    "print('min Length: %d' % (max_len))\n",
    "\n",
    "trainX = encode_sequences(jap_tokenizer, max_len, train[:, 1])\n",
    "testX = encode_sequences(jap_tokenizer, max_len, test[:, 1])\n",
    "\n",
    "# load model\n",
    "model = load_model('models/jap_eng_model3.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)\n",
    "# print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences :  [[11], [4, 7]] \n",
      "\n",
      "word_index :  {'はx助': 1, '彼らx名': 2, 'そこx名': 3, 'にx助': 4, 'すんx動': 5, 'でx助': 6, 'いるx動': 7, '私x名': 8, '何x名': 9, 'もx助': 10, '見x動': 11, 'ませx助': 12, 'んx助': 13}\n"
     ]
    }
   ],
   "source": [
    "t  = Tokenizer()\n",
    "fit_text = [\"彼らx名 はx助 そこx名 にx助 すんx動 でx助 いるx動\",\"私x名 はx助 何x名 もx助 見x動 ませx助 んx助\"]\n",
    "t.fit_on_texts(fit_text)\n",
    "\n",
    "#fit_on_texts fits on sentences when list of sentences is passed to fit_on_texts() function. \n",
    "#ie - fit_on_texts( [ sent1, sent2, sent3,....sentN ] )\n",
    "\n",
    "#Similarly, list of sentences/single sentence in a list must be passed into texts_to_sequences.\n",
    "test_text1 = \"見x動 たx助 ことx名 がx助 ありx動 ますx助\"\n",
    "test_text2 = \"今x名 どこx名 にx助 いるx動 のx助\"\n",
    "sequences = t.texts_to_sequences([test_text1, test_text2])\n",
    "\n",
    "print('sequences : ',sequences,'\\n')\n",
    "\n",
    "print('word_index : ',t.word_index)\n",
    "#texts_to_sequences() returns list of list. ie - [ [] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Japanese: トムに聞いて\n",
      "トムx名 にx助 聞いx動 てx助\n",
      "op\n",
      "can i help\n"
     ]
    }
   ],
   "source": [
    "from kuromojipy.kuromoji_server import KuromojiServer\n",
    "\n",
    "def jap_clean(text):\n",
    "    a = []\n",
    "    with KuromojiServer() as kuro_server:\n",
    "        kuromoji = kuro_server.kuromoji\n",
    "        tokenizer = kuromoji.Tokenizer.builder().build()\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        for token in tokens:\n",
    "            x = token.getSurfaceForm()+\"x\"+token.getAllFeatures()[0]\n",
    "            a.append(x)\n",
    "    # print(a)\n",
    "    return \" \".join(a)\n",
    "\n",
    "\n",
    "ip = input(\"Enter Japanese: \")\n",
    "\n",
    "tokenized_ip = jap_clean(ip)                \n",
    "# tokenized_ip = ' '.join(tokenized_ip)\n",
    "\n",
    "print(tokenized_ip)\n",
    "\n",
    "# ip = encode_sequences(ger_tokenizer, ger_length, tokenized_ip)\n",
    "ip = encode_sequences(jap_tokenizer, jap_length, train[:, 1])\n",
    "\n",
    "# load model\n",
    "# test on some training sequences\n",
    "print('op')\n",
    "\n",
    "translation = predict_sequence(model, eng_tokenizer, ip)\n",
    "\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Enter Japanese: トムに聞いて\n",
      "トムx名 にx助 聞いx動 てx助\n",
      "[[  9   6 173   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]]\n",
      "[[1.8935242e-03 1.4097502e-03 8.2530268e-03 ... 5.3407922e-07\n",
      "  7.8858506e-07 4.3934841e-07]\n",
      " [4.2369766e-03 1.2918802e-04 1.2500399e-03 ... 1.8689406e-08\n",
      "  3.1016107e-08 1.3960496e-08]\n",
      " [5.5843288e-01 2.7029862e-05 5.1095767e-04 ... 9.0137483e-08\n",
      "  1.4070852e-07 6.2224551e-08]\n",
      " ...\n",
      " [9.9892837e-01 5.4638928e-08 9.0995093e-07 ... 7.9754814e-11\n",
      "  1.1659679e-10 6.7172073e-11]\n",
      " [9.9951053e-01 3.2290199e-08 2.9179233e-07 ... 3.5886346e-11\n",
      "  4.9300026e-11 3.0020673e-11]\n",
      " [9.9969971e-01 3.3379774e-08 1.9557221e-07 ... 1.9006064e-11\n",
      "  2.7746277e-11 1.6088081e-11]]\n",
      "[1.8935242e-03 1.4097502e-03 8.2530268e-03 ... 5.3407922e-07 7.8858506e-07\n",
      " 4.3934841e-07]\n",
      "<class 'numpy.ndarray'>\n",
      "[21, 3, 0, 0, 0, 0, 0]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "model = load_model('models/jap_eng_model2.h5')\n",
    "print('Model Loaded')\n",
    "\n",
    "\n",
    "ip = input(\"Enter Japanese: \")\n",
    "\n",
    "tokenized_ip = jap_clean(ip)                \n",
    "# tokenized_ip = ' '.join(tokenized_ip)\n",
    "\n",
    "print(tokenized_ip)\n",
    "\n",
    "# ip = encode_sequences(ger_tokenizer, ger_length, tokenized_ip)\n",
    "ip = encode_sequences(jap_tokenizer, jap_length, [tokenized_ip])\n",
    "print(ip)\n",
    "\n",
    "p = model.predict(ip)[0]\n",
    "print(p)\n",
    "print(p[0])\n",
    "print(type(p[0]))\n",
    "\n",
    "prediction = [argmax(x) for x in p]\n",
    "\n",
    "print(prediction)\n",
    "print(len(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = max(0,1)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
