{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Saved: pkl/english-japanese_2.pkl\n",
      "[go] => [行けx動]\n",
      "[go] => [行きx動 なさいx動]\n",
      "[hi] => [こんにちはx感]\n",
      "[hi] => [もしもしx感]\n",
      "[hi] => [やっx動 ほx動 ーx名]\n",
      "[hi] => [こんにちはx感]\n",
      "[run] => [走れx動]\n",
      "[run] => [走っx動 てx助]\n",
      "[who] => [誰x名]\n",
      "[wow] => [すごいx形]\n",
      "[wow] => [ワォx名]\n",
      "[wow] => [わx助 ぉx名]\n",
      "[wow] => [おx接 ーx名]\n",
      "[fire] => [火事x名 だx助]\n",
      "[fire] => [火事x名]\n",
      "[fire] => [撃てx動]\n",
      "[help] => [助けx動 てx助]\n",
      "[help] => [助けx動 てx助 くれx動]\n",
      "[jump] => [飛び越えろx動]\n",
      "[jump] => [跳べx動]\n",
      "[jump] => [飛び降りろx動]\n",
      "[jump] => [飛び跳ねx動 てx助]\n",
      "[jump] => [ジャンプx名 しx動 てx助]\n",
      "[jump] => [跳べx動]\n",
      "[jump] => [飛び跳ねx動 てx助]\n",
      "[jump] => [ジャンプx名 しx動 てx助]\n",
      "[stop] => [やめろx動]\n",
      "[stop] => [止まれx動]\n",
      "[wait] => [待っx動 てx助]\n",
      "[go on] => [続けx動 てx助]\n",
      "[go on] => [進んx動 でx助]\n",
      "[go on] => [進めx動]\n",
      "[go on] => [続けろx動]\n",
      "[hello] => [こんにちはx感]\n",
      "[hello] => [もしもしx感]\n",
      "[hello] => [こんにちはx感]\n",
      "[hurry] => [急げx動]\n",
      "[i see] => [なるほどx感]\n",
      "[i see] => [なるほどx感 ねx助]\n",
      "[i see] => [わかっx動 たx助]\n",
      "[i see] => [わかりx動 ましx助 たx助]\n",
      "[i see] => [そうx副 ですx助 かx助]\n",
      "[i see] => [そうx副 なx助 んx名 だx助]\n",
      "[i see] => [そっx名 かx助]\n",
      "[i try] => [頑張っx動 てx助 みるx動]\n",
      "[i try] => [やっx動 てx助 みるx動]\n",
      "[i try] => [試しx動 てx助 みるx動]\n",
      "[i try] => [やっx動 てx助 みよx動 うx助]\n",
      "[i try] => [トライx名 しx動 てx助 みるx動]\n",
      "[i won] => [俺x名 のx助 勝ちx名 ーx名]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('%%') for line in  lines]\n",
    "    return pairs\n",
    " \n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "filename = 'data/jap_dataset_2.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-jap pairs\n",
    "pairs = to_pairs(doc)\n",
    "print(type(pairs))\n",
    "\n",
    "clean_pairs = array(pairs)\n",
    "# clean sentences\n",
    "# clean_pairs = clean_pairs(pairs)\n",
    "# print(clean_pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'pkl/english-japanese_3.pkl')\n",
    "# spot check\n",
    "for i in range(50):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: pkl/english-japanese_2-both.pkl\n",
      "Saved: pkl/english-japanese_2-train.pkl\n",
      "Saved: pkl/english-japanese_2-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('pkl/english-japanese_3.pkl')\n",
    "# print(raw_dataset)\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# print(dataset)\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# print(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "# save\n",
    "save_clean_data(dataset, 'pkl/english-japanese_3-both.pkl')\n",
    "save_clean_data(train, 'pkl/english-japanese_3-train.pkl')\n",
    "save_clean_data(test, 'pkl/english-japanese_3-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2716\n",
      "English Max Length: 7\n",
      "Japanese Vocabulary Size: 4147\n",
      "Japanese Max Length: 19\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 19, 256)           1061632   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 7, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 7, 2716)           698012    \n",
      "=================================================================\n",
      "Total params: 2,810,268\n",
      "Trainable params: 2,810,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      " - 48s - loss: 3.8836 - acc: 0.5084 - val_loss: 3.2472 - val_acc: 0.5196\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.24722, saving model to models/jap_eng_model2.h5\n",
      "Epoch 2/20\n",
      " - 49s - loss: 3.0832 - acc: 0.5315 - val_loss: 3.1308 - val_acc: 0.5223\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.24722 to 3.13081, saving model to models/jap_eng_model2.h5\n",
      "Epoch 3/20\n",
      " - 49s - loss: 2.9812 - acc: 0.5356 - val_loss: 3.0740 - val_acc: 0.5250\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.13081 to 3.07399, saving model to models/jap_eng_model2.h5\n",
      "Epoch 4/20\n",
      " - 49s - loss: 2.9128 - acc: 0.5383 - val_loss: 3.0515 - val_acc: 0.5253\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.07399 to 3.05147, saving model to models/jap_eng_model2.h5\n",
      "Epoch 5/20\n",
      " - 49s - loss: 2.8436 - acc: 0.5440 - val_loss: 3.0068 - val_acc: 0.5359\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.05147 to 3.00684, saving model to models/jap_eng_model2.h5\n",
      "Epoch 6/20\n",
      " - 49s - loss: 2.7648 - acc: 0.5525 - val_loss: 2.9554 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.00684 to 2.95536, saving model to models/jap_eng_model2.h5\n",
      "Epoch 7/20\n",
      " - 49s - loss: 2.6674 - acc: 0.5646 - val_loss: 2.8846 - val_acc: 0.5577\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.95536 to 2.88461, saving model to models/jap_eng_model2.h5\n",
      "Epoch 8/20\n",
      " - 49s - loss: 2.5632 - acc: 0.5756 - val_loss: 2.8410 - val_acc: 0.5634\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.88461 to 2.84097, saving model to models/jap_eng_model2.h5\n",
      "Epoch 9/20\n",
      " - 49s - loss: 2.4575 - acc: 0.5849 - val_loss: 2.7596 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.84097 to 2.75964, saving model to models/jap_eng_model2.h5\n",
      "Epoch 10/20\n",
      " - 49s - loss: 2.3552 - acc: 0.5958 - val_loss: 2.6998 - val_acc: 0.5811\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.75964 to 2.69985, saving model to models/jap_eng_model2.h5\n",
      "Epoch 11/20\n",
      " - 49s - loss: 2.2505 - acc: 0.6054 - val_loss: 2.6397 - val_acc: 0.5851\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.69985 to 2.63973, saving model to models/jap_eng_model2.h5\n",
      "Epoch 12/20\n",
      " - 49s - loss: 2.1530 - acc: 0.6125 - val_loss: 2.6063 - val_acc: 0.5870\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.63973 to 2.60630, saving model to models/jap_eng_model2.h5\n",
      "Epoch 13/20\n",
      " - 49s - loss: 2.0597 - acc: 0.6195 - val_loss: 2.5835 - val_acc: 0.5921\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.60630 to 2.58348, saving model to models/jap_eng_model2.h5\n",
      "Epoch 14/20\n",
      " - 49s - loss: 1.9701 - acc: 0.6270 - val_loss: 2.5322 - val_acc: 0.5939\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.58348 to 2.53224, saving model to models/jap_eng_model2.h5\n",
      "Epoch 15/20\n",
      " - 49s - loss: 1.8797 - acc: 0.6356 - val_loss: 2.5019 - val_acc: 0.6013\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.53224 to 2.50188, saving model to models/jap_eng_model2.h5\n",
      "Epoch 16/20\n",
      " - 52s - loss: 1.7861 - acc: 0.6445 - val_loss: 2.4608 - val_acc: 0.5996\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.50188 to 2.46076, saving model to models/jap_eng_model2.h5\n",
      "Epoch 17/20\n",
      " - 49s - loss: 1.6989 - acc: 0.6554 - val_loss: 2.4406 - val_acc: 0.6049\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.46076 to 2.44061, saving model to models/jap_eng_model2.h5\n",
      "Epoch 18/20\n",
      " - 49s - loss: 1.6064 - acc: 0.6674 - val_loss: 2.4070 - val_acc: 0.6101\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.44061 to 2.40698, saving model to models/jap_eng_model2.h5\n",
      "Epoch 19/20\n",
      " - 49s - loss: 1.5210 - acc: 0.6777 - val_loss: 2.3927 - val_acc: 0.6147\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.40698 to 2.39274, saving model to models/jap_eng_model2.h5\n",
      "Epoch 20/20\n",
      " - 49s - loss: 1.4391 - acc: 0.6890 - val_loss: 2.3614 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.39274 to 2.36138, saving model to models/jap_eng_model2.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c10bb674e0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pydot\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "#     y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('pkl/english-japanese_3-both.pkl')\n",
    "train = load_clean_sentences('pkl/english-japanese_3-train.pkl')\n",
    "test = load_clean_sentences('pkl/english-japanese_3-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "jap_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "jap_vocab_size = len(jap_tokenizer.word_index) + 1\n",
    "jap_length = max_length(dataset[:, 1])\n",
    "print('Japanese Vocabulary Size: %d' % jap_vocab_size)\n",
    "print('Japanese Max Length: %d' % (jap_length))\n",
    "\n",
    "max_len = max(jap_length,eng_length)\n",
    "print('Max Length: %d' % (max_len))\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(jap_tokenizer, max_len, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, max_len, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(jap_tokenizer, max_len, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, max_len, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "# define model\n",
    "model = define_model(jap_vocab_size, eng_vocab_size, max_len, max_len, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='japmodel_idk.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'models/jap_eng_model3.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=20, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[手伝おx動 うx助 かx助], target=[can i help], predicted=[can i help]\n",
      "src=[大変x名 、x記 忙しいx形 ですx助], target=[im very busy], predicted=[im very busy]\n",
      "src=[それx名 をx助 見つけx動 なくx助 ちゃx助], target=[i have to find it], predicted=[i want to it]\n",
      "src=[トムx名 をx助 放せx動], target=[let go of tom], predicted=[is tom tom]\n",
      "src=[ちょっとx副 考えx動 させx動 てx助], target=[let me think], predicted=[let me to]\n",
      "src=[私x名 はx助 何x名 もx助 見x動 ませx助 んx助], target=[i dont see anything], predicted=[i cant see it]\n",
      "src=[熱x名 はx助 ありx動 ますx助 かx助], target=[do you have a fever], predicted=[do you have a]\n",
      "src=[話しx動 てx助 もx助 いいx形 ですx助 かx助], target=[may i speak to you], predicted=[may i sit you]\n",
      "src=[間違えx動 ましx助 たx助], target=[ive made a mistake], predicted=[i was a mistake]\n",
      "src=[トムx名 はx助 キャンセルx名 しx動 たx助], target=[tom canceled], predicted=[tom laughed]\n",
      "test\n",
      "src=[今x名 どこx名 にx助 いるx動 のx助], target=[where are you now], predicted=[where are you you]\n",
      "src=[コンサートx名 どうx副 だっx助 たx助], target=[how was the concert], predicted=[how was your trip]\n",
      "src=[まあx副 試しx動 てx助 ごらんx名 よx助], target=[just try it], predicted=[please it it]\n",
      "src=[ビールx名 をx助 おごりx動 ましょx助 うx助], target=[ill buy you a beer], predicted=[be please]\n",
      "src=[よくx副 眠れx動 ませx助 んx助], target=[i cant sleep well], predicted=[i dont to to]\n",
      "src=[私x名 がx助 間違っx動 てx助 いx動 ましx助 たx助], target=[i made a mistake], predicted=[i was wrong]\n",
      "src=[お前x名 にx助 はx助 期待x名 しx動 てx助 いるx動 んx名 だx助 ぞx助], target=[i am counting on you], predicted=[ill be in]\n",
      "src=[彼女x名 はx助 彼x名 のx助 そばx名 にx助 立っx動 てx助 いx動 たx助], target=[she stood by him], predicted=[she held him]\n",
      "src=[彼x名 はx助 正直x名 だx助 とx助 思うx動], target=[i think he is honest], predicted=[he is he good swim]\n",
      "src=[見x動 たx助 ことx名 がx助 ありx動 ますx助], target=[ive seen it], predicted=[i cant you]\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('pkl/english-japanese_3-both.pkl')\n",
    "train = load_clean_sentences('pkl/english-japanese_3-train.pkl')\n",
    "test = load_clean_sentences('pkl/english-japanese_3-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "jap_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "jap_vocab_size = len(jap_tokenizer.word_index) + 1\n",
    "jap_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(jap_tokenizer, jap_length, train[:, 1])\n",
    "testX = encode_sequences(jap_tokenizer, jap_length, test[:, 1])\n",
    "\n",
    "# load model\n",
    "model = load_model('models/jap_eng_model3.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)\n",
    "# print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences :  [[11], [4, 7]] \n",
      "\n",
      "word_index :  {'はx助': 1, '彼らx名': 2, 'そこx名': 3, 'にx助': 4, 'すんx動': 5, 'でx助': 6, 'いるx動': 7, '私x名': 8, '何x名': 9, 'もx助': 10, '見x動': 11, 'ませx助': 12, 'んx助': 13}\n"
     ]
    }
   ],
   "source": [
    "t  = Tokenizer()\n",
    "fit_text = [\"彼らx名 はx助 そこx名 にx助 すんx動 でx助 いるx動\",\"私x名 はx助 何x名 もx助 見x動 ませx助 んx助\"]\n",
    "t.fit_on_texts(fit_text)\n",
    "\n",
    "#fit_on_texts fits on sentences when list of sentences is passed to fit_on_texts() function. \n",
    "#ie - fit_on_texts( [ sent1, sent2, sent3,....sentN ] )\n",
    "\n",
    "#Similarly, list of sentences/single sentence in a list must be passed into texts_to_sequences.\n",
    "test_text1 = \"見x動 たx助 ことx名 がx助 ありx動 ますx助\"\n",
    "test_text2 = \"今x名 どこx名 にx助 いるx動 のx助\"\n",
    "sequences = t.texts_to_sequences([test_text1, test_text2])\n",
    "\n",
    "print('sequences : ',sequences,'\\n')\n",
    "\n",
    "print('word_index : ',t.word_index)\n",
    "#texts_to_sequences() returns list of list. ie - [ [] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Japanese: トムに聞いて\n",
      "トムx名 にx助 聞いx動 てx助\n",
      "op\n",
      "can i help\n"
     ]
    }
   ],
   "source": [
    "from kuromojipy.kuromoji_server import KuromojiServer\n",
    "\n",
    "def jap_clean(text):\n",
    "    a = []\n",
    "    with KuromojiServer() as kuro_server:\n",
    "        kuromoji = kuro_server.kuromoji\n",
    "        tokenizer = kuromoji.Tokenizer.builder().build()\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        for token in tokens:\n",
    "            x = token.getSurfaceForm()+\"x\"+token.getAllFeatures()[0]\n",
    "            a.append(x)\n",
    "    # print(a)\n",
    "    return \" \".join(a)\n",
    "\n",
    "\n",
    "ip = input(\"Enter Japanese: \")\n",
    "\n",
    "tokenized_ip = jap_clean(ip)                \n",
    "# tokenized_ip = ' '.join(tokenized_ip)\n",
    "\n",
    "print(tokenized_ip)\n",
    "\n",
    "# ip = encode_sequences(ger_tokenizer, ger_length, tokenized_ip)\n",
    "ip = encode_sequences(jap_tokenizer, jap_length, train[:, 1])\n",
    "\n",
    "# load model\n",
    "# test on some training sequences\n",
    "print('op')\n",
    "\n",
    "translation = predict_sequence(model, eng_tokenizer, ip)\n",
    "\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Enter Japanese: トムに聞いて\n",
      "トムx名 にx助 聞いx動 てx助\n",
      "[[  9   6 173   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]]\n",
      "[[1.8935242e-03 1.4097502e-03 8.2530268e-03 ... 5.3407922e-07\n",
      "  7.8858506e-07 4.3934841e-07]\n",
      " [4.2369766e-03 1.2918802e-04 1.2500399e-03 ... 1.8689406e-08\n",
      "  3.1016107e-08 1.3960496e-08]\n",
      " [5.5843288e-01 2.7029862e-05 5.1095767e-04 ... 9.0137483e-08\n",
      "  1.4070852e-07 6.2224551e-08]\n",
      " ...\n",
      " [9.9892837e-01 5.4638928e-08 9.0995093e-07 ... 7.9754814e-11\n",
      "  1.1659679e-10 6.7172073e-11]\n",
      " [9.9951053e-01 3.2290199e-08 2.9179233e-07 ... 3.5886346e-11\n",
      "  4.9300026e-11 3.0020673e-11]\n",
      " [9.9969971e-01 3.3379774e-08 1.9557221e-07 ... 1.9006064e-11\n",
      "  2.7746277e-11 1.6088081e-11]]\n",
      "[1.8935242e-03 1.4097502e-03 8.2530268e-03 ... 5.3407922e-07 7.8858506e-07\n",
      " 4.3934841e-07]\n",
      "<class 'numpy.ndarray'>\n",
      "[21, 3, 0, 0, 0, 0, 0]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "model = load_model('models/jap_eng_model2.h5')\n",
    "print('Model Loaded')\n",
    "\n",
    "\n",
    "ip = input(\"Enter Japanese: \")\n",
    "\n",
    "tokenized_ip = jap_clean(ip)                \n",
    "# tokenized_ip = ' '.join(tokenized_ip)\n",
    "\n",
    "print(tokenized_ip)\n",
    "\n",
    "# ip = encode_sequences(ger_tokenizer, ger_length, tokenized_ip)\n",
    "ip = encode_sequences(jap_tokenizer, jap_length, [tokenized_ip])\n",
    "print(ip)\n",
    "\n",
    "p = model.predict(ip)[0]\n",
    "print(p)\n",
    "print(p[0])\n",
    "print(type(p[0]))\n",
    "\n",
    "prediction = [argmax(x) for x in p]\n",
    "\n",
    "print(prediction)\n",
    "print(len(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = max(0,1)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
