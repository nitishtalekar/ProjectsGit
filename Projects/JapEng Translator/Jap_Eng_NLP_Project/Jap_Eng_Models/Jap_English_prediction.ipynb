{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: pkl1/english-japanese_5.pkl\n",
      "[go] => [行け。動]\n",
      "[go] => [行き。動 なさい。動]\n",
      "[hi] => [こんにちは。感]\n",
      "[hi] => [もしもし。感]\n",
      "[hi] => [やっ。動 ほ。動 ー。名]\n",
      "[hi] => [こんにちは。感]\n",
      "[run] => [走れ。動]\n",
      "[run] => [走っ。動 て。助]\n",
      "[who] => [誰。名]\n",
      "[wow] => [すごい。形]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import tinysegmenter\n",
    " \n",
    "# load doc into memory\n",
    "    def load_doc(filename)\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('%%') for line in  lines]\n",
    "    return pairs\n",
    " \n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "\n",
    "filename = 'jap_dataset2.txt'\n",
    "doc = load_doc(filename)\n",
    "\n",
    "pairs = to_pairs(doc)\n",
    "\n",
    "clean_pairs = array(pairs)\n",
    "\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'pkl1/english-japanese_5.pkl')\n",
    "\n",
    "for i in range(10):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: pkl1/english-japanese_5-both.pkl\n",
      "Saved: pkl1/english-japanese_5-train.pkl\n",
      "Saved: pkl1/english-japanese_5-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "raw_dataset = load_clean_sentences('pkl1/english-japanese_3.pkl')\n",
    "\n",
    "\n",
    "# dataset size\n",
    "n_sentences = 15000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "\n",
    "# split into train/test\n",
    "train, test = dataset[:13500], dataset[13500:]\n",
    "# save\n",
    "save_clean_data(dataset, 'pkl1/english-japanese_5-both.pkl')\n",
    "save_clean_data(train, 'pkl1/english-japanese_5-train.pkl')\n",
    "save_clean_data(test, 'pkl1/english-japanese_5-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 3580\n",
      "English Max Length: 7\n",
      "Japanese Vocabulary Size: 5625\n",
      "Japanese Max Length: 19\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 19, 256)           1440000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 19, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 19, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 19, 3580)          920060    \n",
      "=================================================================\n",
      "Total params: 3,410,684\n",
      "Trainable params: 3,410,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13500 samples, validate on 1500 samples\n",
      "Epoch 1/30\n",
      " - 119s - loss: 1.8216 - accuracy: 0.7937 - val_loss: 1.3768 - val_accuracy: 0.8015\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.80154, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 2/30\n",
      " - 130s - loss: 1.3163 - accuracy: 0.8039 - val_loss: 1.2956 - val_accuracy: 0.8065\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.80154 to 0.80649, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 3/30\n",
      " - 117s - loss: 1.2662 - accuracy: 0.8067 - val_loss: 1.2732 - val_accuracy: 0.8065\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.80649\n",
      "Epoch 4/30\n",
      " - 118s - loss: 1.2425 - accuracy: 0.8075 - val_loss: 1.2641 - val_accuracy: 0.8060\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.80649\n",
      "Epoch 5/30\n",
      " - 124s - loss: 1.2167 - accuracy: 0.8091 - val_loss: 1.2437 - val_accuracy: 0.8087\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.80649 to 0.80874, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 6/30\n",
      " - 119s - loss: 1.1774 - accuracy: 0.8153 - val_loss: 1.2046 - val_accuracy: 0.8158\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.80874 to 0.81575, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 7/30\n",
      " - 123s - loss: 1.1282 - accuracy: 0.8200 - val_loss: 1.1651 - val_accuracy: 0.8202\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.81575 to 0.82025, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 8/30\n",
      " - 117s - loss: 1.0826 - accuracy: 0.8246 - val_loss: 1.1327 - val_accuracy: 0.8241\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.82025 to 0.82411, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 9/30\n",
      " - 114s - loss: 1.0372 - accuracy: 0.8297 - val_loss: 1.0992 - val_accuracy: 0.8268\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.82411 to 0.82684, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 10/30\n",
      " - 114s - loss: 0.9890 - accuracy: 0.8340 - val_loss: 1.0692 - val_accuracy: 0.8298\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.82684 to 0.82975, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 11/30\n",
      " - 117s - loss: 0.9412 - accuracy: 0.8380 - val_loss: 1.0406 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.82975 to 0.83158, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 12/30\n",
      " - 124s - loss: 0.8935 - accuracy: 0.8418 - val_loss: 1.0166 - val_accuracy: 0.8341\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.83158 to 0.83407, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 13/30\n",
      " - 125s - loss: 0.8556 - accuracy: 0.8450 - val_loss: 0.9862 - val_accuracy: 0.8361\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.83407 to 0.83611, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 14/30\n",
      " - 112s - loss: 0.8069 - accuracy: 0.8489 - val_loss: 0.9619 - val_accuracy: 0.8394\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.83611 to 0.83940, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 15/30\n",
      " - 129s - loss: 0.7623 - accuracy: 0.8531 - val_loss: 0.9450 - val_accuracy: 0.8414\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.83940 to 0.84144, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 16/30\n",
      " - 119s - loss: 0.7217 - accuracy: 0.8573 - val_loss: 0.9322 - val_accuracy: 0.8443\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.84144 to 0.84428, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 17/30\n",
      " - 119s - loss: 0.6834 - accuracy: 0.8612 - val_loss: 0.9193 - val_accuracy: 0.8453\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.84428 to 0.84526, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 18/30\n",
      " - 123s - loss: 0.6450 - accuracy: 0.8652 - val_loss: 0.9050 - val_accuracy: 0.8452\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.84526\n",
      "Epoch 19/30\n",
      " - 124s - loss: 0.6085 - accuracy: 0.8697 - val_loss: 0.8904 - val_accuracy: 0.8476\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.84526 to 0.84758, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 20/30\n",
      " - 131s - loss: 0.5733 - accuracy: 0.8744 - val_loss: 0.8856 - val_accuracy: 0.8498\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.84758 to 0.84979, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 21/30\n",
      " - 123s - loss: 0.5393 - accuracy: 0.8789 - val_loss: 0.8715 - val_accuracy: 0.8508\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.84979 to 0.85077, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 22/30\n",
      " - 119s - loss: 0.5063 - accuracy: 0.8843 - val_loss: 0.8679 - val_accuracy: 0.8517\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.85077 to 0.85172, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 23/30\n",
      " - 117s - loss: 0.4753 - accuracy: 0.8893 - val_loss: 0.8633 - val_accuracy: 0.8536\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.85172 to 0.85358, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 24/30\n",
      " - 117s - loss: 0.4459 - accuracy: 0.8950 - val_loss: 0.8583 - val_accuracy: 0.8535\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.85358\n",
      "Epoch 25/30\n",
      " - 116s - loss: 0.4178 - accuracy: 0.8998 - val_loss: 0.8538 - val_accuracy: 0.8553\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.85358 to 0.85533, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 26/30\n",
      " - 116s - loss: 0.3917 - accuracy: 0.9052 - val_loss: 0.8499 - val_accuracy: 0.8539\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.85533\n",
      "Epoch 27/30\n",
      " - 115s - loss: 0.3670 - accuracy: 0.9105 - val_loss: 0.8492 - val_accuracy: 0.8556\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.85533 to 0.85565, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 28/30\n",
      " - 111s - loss: 0.3435 - accuracy: 0.9151 - val_loss: 0.8471 - val_accuracy: 0.8577\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.85565 to 0.85768, saving model to model1/japmodel_simple_5.h5\n",
      "Epoch 29/30\n",
      " - 110s - loss: 0.3211 - accuracy: 0.9205 - val_loss: 0.8536 - val_accuracy: 0.8563\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.85768\n",
      "Epoch 30/30\n",
      " - 108s - loss: 0.3007 - accuracy: 0.9244 - val_loss: 0.8513 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.85768 to 0.85825, saving model to model1/japmodel_simple_5.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x257d479278>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('pkl1/english-japanese_5-both.pkl')\n",
    "train = load_clean_sentences('pkl1/english-japanese_5-train.pkl')\n",
    "test = load_clean_sentences('pkl1/english-japanese_5-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare japanese tokenizer\n",
    "jpn_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "jpn_vocab_size = len(jpn_tokenizer.word_index) + 1\n",
    "jpn_length = max_length(dataset[:, 1])\n",
    "print('Japanese Vocabulary Size: %d' % jpn_vocab_size)\n",
    "print('Japanese Max Length: %d' % (jpn_length))\n",
    "\n",
    "ml = max(eng_length, jpn_length)\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(jpn_tokenizer, ml, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, ml, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(jpn_tokenizer, ml, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, ml, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "# define model\n",
    "model = define_model(jpn_vocab_size, eng_vocab_size, ml, ml, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='japmodel_simple2.png', show_shapes=True)\n",
    "\n",
    "# fit model\n",
    "filename = 'model1/japmodel_simple_5.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[その。連 工場。名 は。助 玩具。名 を。助 製造。名 し。動 て。助 いる。動], target=[that factory makes toys], predicted=[the factory makes toys]\n",
      "src=[撃て。動], target=[shoot], predicted=[welcome]\n",
      "src=[彼。名 は。助 ぐっすり。副 眠っ。動 て。助 い。動 た。助], target=[he was fast asleep], predicted=[he was asleep]\n",
      "src=[だから。接 黙れ。動 って。助 ば。助], target=[i said shut up], predicted=[i said up up]\n",
      "src=[砂糖。名 が。助 ない。形 よ。助], target=[theres no sugar], predicted=[we have sugar sugar]\n",
      "src=[うん。感 と。助 言っ。動 て。助 よ。助], target=[just say yes], predicted=[say say yes]\n",
      "src=[よく。副 食べ。動 られる。動 ね。助 、。記 そんなに。副], target=[how can you eat so much], predicted=[why can you eat eat]\n",
      "src=[ハンガー。名 を。助 ください。動], target=[i need some hangers], predicted=[i need some hangers]\n",
      "src=[これ。名 使っ。動 て。助], target=[take this], predicted=[take this]\n",
      "src=[私。名 は。助 もう。副 彼。名 に。助 会わ。動 ない。助 だろ。助 う。助], target=[i will never see him], predicted=[i wont see him him]\n",
      "test\n",
      "src=[トム。名 は。助 卵。名 を。助 1。名 つも。動 買わ。動 なかっ。助 た。助], target=[tom didnt buy any eggs], predicted=[tom needs to to visa]\n",
      "src=[この。連 暗号。名 は。助 私。名 に。助 は。助 解読。名 でき。動 ない。助], target=[i cant break this code], predicted=[this can hurt this]\n",
      "src=[もう一度。副 やっ。動 て。助 み。動 て。助], target=[try again], predicted=[try it another]\n",
      "src=[楽屋。名 口。名 は。助 どこ。名 です。助 か。助], target=[where is the stage door], predicted=[where is your]\n",
      "src=[持っ。動 て。助 い。動 ます。助], target=[i have it], predicted=[i have]\n",
      "src=[覚え。動 て。助 い。動 ない。助 ん。名 だ。助], target=[i dont remember], predicted=[i dont remember]\n",
      "src=[私。名 は。助 UFO。名 を。助 見。動 た。助 こと。名 が。助 あり。動 ます。助], target=[i have seen a ufo], predicted=[i seen it see]\n",
      "src=[興奮。名 し。動 てる。動], target=[are you excited], predicted=[are you]\n",
      "src=[私。名 たち。名 は。助 全員。名 フランス語。名 を。助 喋る。動], target=[all of us speak french], predicted=[we live speak french]\n",
      "src=[彼。名 なら。助 できる。動], target=[he can do it], predicted=[he can do it]\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('pkl1/english-japanese_5-both.pkl')\n",
    "train = load_clean_sentences('pkl1/english-japanese_5-train.pkl')\n",
    "test = load_clean_sentences('pkl1/english-japanese_5-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "# prepare japanese tokenizer\n",
    "jpn_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "jpn_vocab_size = len(jpn_tokenizer.word_index) + 1\n",
    "jpn_length = max_length(dataset[:, 1])\n",
    "\n",
    "ml = max(eng_length, jpn_length)\n",
    "\n",
    "# prepare data\n",
    "trainX = encode_sequences(jpn_tokenizer, ml, train[:, 1])\n",
    "testX = encode_sequences(jpn_tokenizer, ml, test[:, 1])\n",
    "\n",
    "# load model\n",
    "model = load_model('model1/japmodel_simple_5.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter no: タクシー。名 を。助 呼ん。動 で。助 あげ。動 ましょ。助 う。助\n",
      "ill call you a taxi\n"
     ]
    }
   ],
   "source": [
    "s = input('Enter jap: ')\n",
    "\n",
    "x = array(s).reshape(1,)\n",
    "\n",
    "trainX = encode_sequences(jpn_tokenizer, ml, x)\n",
    "\n",
    "translation = predict_sequence(model,eng_tokenizer,trainX)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
