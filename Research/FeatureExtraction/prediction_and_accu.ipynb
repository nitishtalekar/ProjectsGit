{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traind = pd.read_csv('traindata.csv')\n",
    "trainl  = pd.read_csv('trainlabel.csv')\n",
    "testd = pd.read_csv('testdata.csv')\n",
    "testl = pd.read_csv('testlabel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1         2         3         4         5         6         7  \\\n",
      "0      0.011746  0.487874  0.147778  0.370302  0.052845  0.003613  0.615610   \n",
      "1      0.009512  0.490273  0.247965  0.527739  0.034093  0.000676  0.709627   \n",
      "2      0.050723  0.446118  0.171614  0.317674  0.232701  0.076288  0.895170   \n",
      "3      0.016577  0.482716  0.075212  0.257109  0.078926  0.005967  0.701811   \n",
      "4      0.042807  0.454620  0.149278  0.291658  0.115798  0.006627  0.803072   \n",
      "5      0.009509  0.490289  0.174600  0.398439  0.059641  0.001710  0.600149   \n",
      "6      0.014903  0.484178  0.153474  0.412524  0.028480  0.001185  0.639693   \n",
      "7      0.026783  0.470589  0.408015  0.605206  0.037874  0.000970  0.511026   \n",
      "8      0.042542  0.455086  0.118718  0.276264  0.197291  0.014940  0.813614   \n",
      "9      0.036282  0.462323  0.090859  0.241383  0.167017  0.022154  0.876333   \n",
      "10     0.009411  0.490094  0.027823  0.175979  0.081565  0.008592  0.549157   \n",
      "11     0.040472  0.458571  0.335097  0.539918  0.060294  0.002103  0.666822   \n",
      "12     0.046474  0.452275  0.177066  0.358255  0.230853  0.047420  0.811880   \n",
      "13     0.112051  0.388391  0.111754  0.239743  0.219913  0.038532  0.916920   \n",
      "14     0.008305  0.491636  0.193105  0.436982  0.068731  0.002261  0.647262   \n",
      "15     0.037956  0.457915  0.084187  0.263422  0.171679  0.015125  0.823815   \n",
      "16     0.073064  0.429271  0.119404  0.308809  0.096333  0.007582  0.900999   \n",
      "17     0.047537  0.449672  0.101392  0.278352  0.098377  0.006330  0.789388   \n",
      "18     0.034182  0.463722  0.104409  0.274756  0.115834  0.005227  0.875766   \n",
      "19     0.014190  0.484880  0.132010  0.377212  0.067413  0.002671  0.741568   \n",
      "20     0.012253  0.486998  0.131317  0.292769  0.195108  0.035029  0.700595   \n",
      "21     0.021304  0.477190  0.040278  0.175217  0.129272  0.011339  0.863420   \n",
      "22     0.006156  0.494294  0.100363  0.307185  0.050749  0.004549  0.615230   \n",
      "23     0.010755  0.489006  0.079147  0.286024  0.057501  0.002767  0.660587   \n",
      "24     0.014755  0.483251  0.232077  0.506993  0.025231  0.001374  0.427462   \n",
      "25     0.037361  0.462153  0.124477  0.360012  0.034889  0.001919  0.810540   \n",
      "26     0.030901  0.466315  0.068069  0.271326  0.053593  0.002286  0.836457   \n",
      "27     0.013297  0.485986  0.179967  0.435574  0.031759  0.001524  0.367841   \n",
      "28     0.014029  0.484503  0.182339  0.401289  0.053765  0.002066  0.645032   \n",
      "29     0.125936  0.374150  0.327862  0.423510  0.176769  0.017775  0.837142   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "49970  0.058407  0.440179  0.306603  0.448416  0.353640  0.227782  0.815100   \n",
      "49971  0.032595  0.466836  0.212060  0.419948  0.077415  0.003322  0.787102   \n",
      "49972  0.023073  0.475281  0.143709  0.290797  0.140838  0.016963  0.634809   \n",
      "49973  0.018687  0.479985  0.107773  0.327285  0.062420  0.002527  0.778341   \n",
      "49974  0.051253  0.446115  0.270868  0.473338  0.085210  0.006637  0.624455   \n",
      "49975  0.089056  0.410493  0.143309  0.360724  0.129041  0.019975  0.696552   \n",
      "49976  0.033766  0.466292  0.084753  0.286455  0.077211  0.002886  0.768556   \n",
      "49977  0.015701  0.484344  0.261996  0.515841  0.132803  0.065093  0.719077   \n",
      "49978  0.040688  0.455065  0.254878  0.422566  0.187169  0.060460  0.791452   \n",
      "49979  0.046867  0.452970  0.277895  0.561328  0.040722  0.004119  0.722924   \n",
      "49980  0.024029  0.475609  0.162489  0.404918  0.033773  0.001211  0.743364   \n",
      "49981  0.016623  0.482340  0.049963  0.206767  0.095942  0.007325  0.792327   \n",
      "49982  0.050864  0.443968  0.018701  0.115911  0.214792  0.021534  0.863210   \n",
      "49983  0.023276  0.475954  0.234386  0.518526  0.032735  0.000945  0.574994   \n",
      "49984  0.012963  0.487585  0.189967  0.459071  0.033573  0.002969  0.210188   \n",
      "49985  0.044305  0.453484  0.029566  0.111901  0.354564  0.039713  0.955439   \n",
      "49986  0.016466  0.482697  0.221792  0.458491  0.124806  0.010234  0.579999   \n",
      "49987  0.007242  0.493102  0.096060  0.338951  0.048941  0.003659  0.412311   \n",
      "49988  0.010614  0.488787  0.083988  0.251889  0.100684  0.005740  0.613489   \n",
      "49989  0.064517  0.437683  0.315726  0.472957  0.276273  0.123535  0.786474   \n",
      "49990  0.045941  0.449577  0.163839  0.326407  0.154880  0.029592  0.856453   \n",
      "49991  0.059018  0.439275  0.094473  0.211689  0.463748  0.214043  0.842186   \n",
      "49992  0.020830  0.478751  0.062751  0.191125  0.299916  0.039934  0.865759   \n",
      "49993  0.024591  0.472516  0.097620  0.296215  0.070213  0.002515  0.856689   \n",
      "49994  0.059538  0.429983  0.060138  0.094833  0.580232  0.155370  0.719615   \n",
      "49995  0.032952  0.464005  0.029666  0.118562  0.177791  0.014304  0.966337   \n",
      "49996  0.079851  0.419581  0.085317  0.238865  0.137875  0.010386  0.895350   \n",
      "49997  0.051145  0.446889  0.148847  0.323192  0.106908  0.004840  0.869523   \n",
      "49998  0.047217  0.451180  0.113969  0.277043  0.142083  0.005249  0.844124   \n",
      "49999  0.035670  0.463147  0.135784  0.343276  0.074867  0.001944  0.813258   \n",
      "\n",
      "              8         9  \n",
      "0      0.626888  0.244665  \n",
      "1      0.433525  0.380612  \n",
      "2      0.416196  0.344619  \n",
      "3      0.697956  0.172308  \n",
      "4      0.778575  0.108338  \n",
      "5      0.703056  0.171768  \n",
      "6      0.314125  0.478387  \n",
      "7      0.494491  0.366553  \n",
      "8      0.258903  0.509369  \n",
      "9      0.547650  0.329409  \n",
      "10     0.390890  0.427828  \n",
      "11     0.246045  0.587478  \n",
      "12     0.314080  0.445051  \n",
      "13     0.488895  0.377886  \n",
      "14     0.767823  0.113582  \n",
      "15     0.259700  0.540213  \n",
      "16     0.188380  0.640909  \n",
      "17     0.669634  0.181096  \n",
      "18     0.195087  0.631909  \n",
      "19     0.262836  0.549400  \n",
      "20     0.307133  0.511573  \n",
      "21     0.166318  0.636511  \n",
      "22     0.758901  0.120316  \n",
      "23     0.325209  0.491021  \n",
      "24     0.675346  0.189298  \n",
      "25     0.318117  0.427616  \n",
      "26     0.304893  0.495202  \n",
      "27     0.821981  0.050750  \n",
      "28     0.738634  0.149912  \n",
      "29     0.299873  0.468906  \n",
      "...         ...       ...  \n",
      "49970  0.226824  0.572596  \n",
      "49971  0.506055  0.326700  \n",
      "49972  0.735487  0.149835  \n",
      "49973  0.401323  0.435584  \n",
      "49974  0.403564  0.416353  \n",
      "49975  0.575747  0.258062  \n",
      "49976  0.334781  0.464884  \n",
      "49977  0.152747  0.682464  \n",
      "49978  0.306289  0.500700  \n",
      "49979  0.433595  0.408296  \n",
      "49980  0.307420  0.498884  \n",
      "49981  0.173477  0.636105  \n",
      "49982  0.506731  0.335579  \n",
      "49983  0.302620  0.519529  \n",
      "49984  0.584116  0.298568  \n",
      "49985  0.171839  0.613317  \n",
      "49986  0.475289  0.378903  \n",
      "49987  0.524524  0.305903  \n",
      "49988  0.454483  0.354372  \n",
      "49989  0.285784  0.490592  \n",
      "49990  0.247330  0.543550  \n",
      "49991  0.260305  0.543584  \n",
      "49992  0.716235  0.149492  \n",
      "49993  0.305824  0.495018  \n",
      "49994  0.680036  0.201045  \n",
      "49995  0.132912  0.685447  \n",
      "49996  0.367893  0.342089  \n",
      "49997  0.521638  0.318298  \n",
      "49998  0.135321  0.684533  \n",
      "49999  0.382343  0.425074  \n",
      "\n",
      "[50000 rows x 9 columns]\n",
      "       0\n",
      "0      6\n",
      "1      9\n",
      "2      9\n",
      "3      4\n",
      "4      1\n",
      "5      1\n",
      "6      2\n",
      "7      7\n",
      "8      8\n",
      "9      3\n",
      "10     4\n",
      "11     7\n",
      "12     7\n",
      "13     2\n",
      "14     9\n",
      "15     9\n",
      "16     9\n",
      "17     3\n",
      "18     2\n",
      "19     6\n",
      "20     4\n",
      "21     3\n",
      "22     6\n",
      "23     6\n",
      "24     2\n",
      "25     6\n",
      "26     3\n",
      "27     5\n",
      "28     4\n",
      "29     0\n",
      "...   ..\n",
      "49970  3\n",
      "49971  9\n",
      "49972  2\n",
      "49973  1\n",
      "49974  4\n",
      "49975  3\n",
      "49976  8\n",
      "49977  1\n",
      "49978  7\n",
      "49979  3\n",
      "49980  5\n",
      "49981  4\n",
      "49982  3\n",
      "49983  3\n",
      "49984  4\n",
      "49985  8\n",
      "49986  7\n",
      "49987  2\n",
      "49988  5\n",
      "49989  1\n",
      "49990  4\n",
      "49991  2\n",
      "49992  0\n",
      "49993  1\n",
      "49994  0\n",
      "49995  2\n",
      "49996  6\n",
      "49997  9\n",
      "49998  1\n",
      "49999  1\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "             1         2         3         4         5         6         7  \\\n",
      "0     0.011520  0.488263  0.181202  0.410403  0.070084  0.004176  0.453139   \n",
      "1     0.015760  0.483556  0.094555  0.254474  0.214207  0.045351  0.937609   \n",
      "2     0.039579  0.457500  0.069028  0.252187  0.070890  0.002431  0.930849   \n",
      "3     0.021817  0.477848  0.064228  0.191638  0.184061  0.010282  0.906821   \n",
      "4     0.010601  0.489261  0.180007  0.467969  0.017795  0.001697  0.490758   \n",
      "5     0.023316  0.474416  0.109239  0.323623  0.052857  0.002225  0.631438   \n",
      "6     0.021929  0.478028  0.255874  0.443163  0.082152  0.005913  0.609332   \n",
      "7     0.043622  0.452857  0.051928  0.202343  0.090614  0.005150  0.831253   \n",
      "8     0.026829  0.471394  0.123609  0.356217  0.064300  0.003205  0.677832   \n",
      "9     0.017631  0.480763  0.228248  0.475343  0.059336  0.003474  0.751761   \n",
      "10    0.053664  0.443725  0.078653  0.224614  0.284640  0.059908  0.541342   \n",
      "11    0.011537  0.487520  0.208869  0.454017  0.078076  0.004062  0.782289   \n",
      "12    0.027888  0.471421  0.130846  0.344204  0.072825  0.002587  0.783738   \n",
      "13    0.025867  0.471933  0.450859  0.663203  0.042543  0.003087  0.645489   \n",
      "14    0.005256  0.495085  0.189670  0.427123  0.057917  0.001919  0.698531   \n",
      "15    0.017987  0.480765  0.179909  0.427189  0.054108  0.002767  0.642311   \n",
      "16    0.060569  0.439558  0.261029  0.525220  0.046481  0.004661  0.805491   \n",
      "17    0.017225  0.482004  0.175295  0.394910  0.068606  0.004907  0.708926   \n",
      "18    0.079848  0.421628  0.108142  0.284649  0.102723  0.010556  0.946932   \n",
      "19    0.025160  0.472854  0.191531  0.448615  0.031137  0.001008  0.713660   \n",
      "20    0.039491  0.457049  0.287235  0.483893  0.075651  0.004391  0.683950   \n",
      "21    0.014638  0.483806  0.247801  0.241168  0.550660  0.223513  0.795825   \n",
      "22    0.026881  0.471667  0.020142  0.132498  0.143594  0.017442  0.579715   \n",
      "23    0.008050  0.491767  0.209369  0.428631  0.064547  0.003075  0.682928   \n",
      "24    0.032478  0.465572  0.119890  0.361238  0.043146  0.002018  0.657610   \n",
      "25    0.011952  0.487258  0.206395  0.419848  0.066001  0.004176  0.672596   \n",
      "26    0.053768  0.443315  0.218320  0.440528  0.042453  0.002909  0.680543   \n",
      "27    0.042084  0.457143  0.131941  0.340873  0.135981  0.014763  0.836389   \n",
      "28    0.040295  0.460692  0.244092  0.517714  0.031942  0.001248  0.752145   \n",
      "29    0.009587  0.490271  0.108583  0.365464  0.037651  0.002237  0.590530   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "9970  0.148179  0.352515  0.044531  0.156625  0.165831  0.009804  0.855929   \n",
      "9971  0.053814  0.444533  0.248029  0.471002  0.084690  0.006805  0.809269   \n",
      "9972  0.036674  0.460248  0.101180  0.300905  0.063363  0.001993  0.883118   \n",
      "9973  0.053121  0.448580  0.023078  0.074518  0.624937  0.254824  0.956114   \n",
      "9974  0.025841  0.472806  0.178929  0.397876  0.070947  0.002188  0.720256   \n",
      "9975  0.027650  0.471179  0.377049  0.506181  0.250178  0.119913  0.738388   \n",
      "9976  0.022491  0.476256  0.175064  0.457430  0.032731  0.001561  0.600108   \n",
      "9977  0.039167  0.456515  0.292570  0.459883  0.291551  0.154012  0.837909   \n",
      "9978  0.017876  0.481212  0.077530  0.229353  0.098136  0.005359  0.800862   \n",
      "9979  0.025530  0.472974  0.058284  0.231458  0.128415  0.006616  0.605131   \n",
      "9980  0.125599  0.375540  0.218348  0.271426  0.223409  0.026784  0.864244   \n",
      "9981  0.011520  0.487571  0.142266  0.404968  0.029839  0.001932  0.641389   \n",
      "9982  0.022347  0.476253  0.192203  0.451117  0.031859  0.001735  0.798697   \n",
      "9983  0.018769  0.480547  0.061606  0.215252  0.098120  0.005293  0.837186   \n",
      "9984  0.023590  0.476122  0.119375  0.332422  0.105090  0.005784  0.715984   \n",
      "9985  0.016270  0.482086  0.455846  0.679824  0.030081  0.003474  0.518258   \n",
      "9986  0.030060  0.467320  0.132827  0.298717  0.076729  0.005271  0.719814   \n",
      "9987  0.037649  0.462186  0.091228  0.215583  0.117521  0.007139  0.739032   \n",
      "9988  0.041728  0.458674  0.115624  0.233380  0.176872  0.014949  0.903207   \n",
      "9989  0.033671  0.463790  0.076181  0.244631  0.092927  0.004233  0.837485   \n",
      "9990  0.019148  0.480420  0.439158  0.531567  0.067888  0.003867  0.728847   \n",
      "9991  0.055407  0.444704  0.138033  0.269056  0.268184  0.024199  0.770420   \n",
      "9992  0.074918  0.421188  0.096697  0.288063  0.090025  0.003474  0.923439   \n",
      "9993  0.038781  0.461205  0.366841  0.610227  0.047204  0.001461  0.651370   \n",
      "9994  0.048963  0.446473  0.250340  0.542073  0.022304  0.000728  0.645351   \n",
      "9995  0.039644  0.457089  0.297625  0.482568  0.119969  0.009766  0.768256   \n",
      "9996  0.020728  0.477912  0.133662  0.383443  0.044953  0.001932  0.749960   \n",
      "9997  0.075259  0.423686  0.188516  0.375240  0.221347  0.042227  0.748464   \n",
      "9998  0.038388  0.459638  0.247162  0.463147  0.074324  0.002838  0.639608   \n",
      "9999  0.023872  0.475486  0.141066  0.349042  0.098327  0.003993  0.696657   \n",
      "\n",
      "             8         9  \n",
      "0     0.582339  0.297776  \n",
      "1     0.197095  0.593463  \n",
      "2     0.378695  0.441205  \n",
      "3     0.211164  0.556711  \n",
      "4     0.274099  0.541097  \n",
      "5     0.541842  0.300852  \n",
      "6     0.579426  0.309952  \n",
      "7     0.683037  0.202715  \n",
      "8     0.140514  0.674856  \n",
      "9     0.223128  0.609591  \n",
      "10    0.197046  0.632770  \n",
      "11    0.431451  0.405520  \n",
      "12    0.480823  0.355210  \n",
      "13    0.621532  0.250346  \n",
      "14    0.612141  0.252943  \n",
      "15    0.328046  0.499954  \n",
      "16    0.257386  0.565621  \n",
      "17    0.362357  0.446384  \n",
      "18    0.185935  0.630707  \n",
      "19    0.441040  0.395899  \n",
      "20    0.553029  0.316235  \n",
      "21    0.617396  0.238142  \n",
      "22    0.476724  0.370834  \n",
      "23    0.606297  0.263516  \n",
      "24    0.179339  0.658974  \n",
      "25    0.593913  0.273488  \n",
      "26    0.582421  0.298586  \n",
      "27    0.286012  0.524302  \n",
      "28    0.465486  0.347093  \n",
      "29    0.376199  0.449735  \n",
      "...        ...       ...  \n",
      "9970  0.455326  0.357985  \n",
      "9971  0.309560  0.501323  \n",
      "9972  0.662671  0.200701  \n",
      "9973  0.245102  0.662781  \n",
      "9974  0.672407  0.204393  \n",
      "9975  0.346397  0.442067  \n",
      "9976  0.113637  0.735035  \n",
      "9977  0.582855  0.262642  \n",
      "9978  0.708514  0.179134  \n",
      "9979  0.248874  0.579625  \n",
      "9980  0.374983  0.374790  \n",
      "9981  0.353427  0.454096  \n",
      "9982  0.623256  0.220616  \n",
      "9983  0.637456  0.221631  \n",
      "9984  0.241546  0.547070  \n",
      "9985  0.670159  0.201128  \n",
      "9986  0.276064  0.493694  \n",
      "9987  0.506051  0.327205  \n",
      "9988  0.394316  0.378303  \n",
      "9989  0.482221  0.329653  \n",
      "9990  0.496983  0.361755  \n",
      "9991  0.802852  0.078744  \n",
      "9992  0.351324  0.444186  \n",
      "9993  0.552057  0.297755  \n",
      "9994  0.469833  0.378785  \n",
      "9995  0.240306  0.558745  \n",
      "9996  0.278100  0.529702  \n",
      "9997  0.664532  0.194018  \n",
      "9998  0.451507  0.381099  \n",
      "9999  0.622900  0.229586  \n",
      "\n",
      "[10000 rows x 9 columns]\n",
      "      0\n",
      "0     1\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "11    1\n",
      "12    1\n",
      "13    1\n",
      "14    1\n",
      "15    1\n",
      "16    1\n",
      "17    1\n",
      "18    1\n",
      "19    1\n",
      "20    1\n",
      "21    1\n",
      "22    1\n",
      "23    1\n",
      "24    1\n",
      "25    1\n",
      "26    1\n",
      "27    1\n",
      "28    1\n",
      "29    1\n",
      "...  ..\n",
      "9970  1\n",
      "9971  1\n",
      "9972  1\n",
      "9973  1\n",
      "9974  1\n",
      "9975  1\n",
      "9976  1\n",
      "9977  1\n",
      "9978  1\n",
      "9979  1\n",
      "9980  1\n",
      "9981  1\n",
      "9982  1\n",
      "9983  1\n",
      "9984  1\n",
      "9985  1\n",
      "9986  1\n",
      "9987  1\n",
      "9988  1\n",
      "9989  1\n",
      "9990  1\n",
      "9991  1\n",
      "9992  1\n",
      "9993  1\n",
      "9994  1\n",
      "9995  1\n",
      "9996  1\n",
      "9997  1\n",
      "9998  1\n",
      "9999  1\n",
      "\n",
      "[10000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# traind.drop(['Unnamed: 0'],1, inplace=True)\n",
    "trainl.drop(['1'],1, inplace=True)\n",
    "# testd.drop(['Unnamed: 0'],1, inplace=True)\n",
    "testl.drop(['1'],1, inplace=True)\n",
    "# trainl = trainl.iloc[1:]\n",
    "# testl = testl.iloc[1:]\n",
    "print(traind)\n",
    "print(trainl)\n",
    "print(testd)\n",
    "print(testl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print (\"Trained model in \",end - start,\" seconds\")\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    # Print and return results\n",
    "    print (\"Made predictions in \",end - start,\" seconds.\")\n",
    "#     print(y_pred,len(y_pred))\n",
    "#     print(target,len(target))\n",
    "    \n",
    "    return sum(target['0'] == y_pred) / float(len(y_pred))\n",
    "\n",
    "\n",
    "def pred(clf, features):\n",
    "    y_pred = clf.predict(features)\n",
    "    global l \n",
    "    l = [i for i in y_pred]\n",
    "    print(y_pred)\n",
    "    print(type(y_pred))\n",
    "    print(l)\n",
    "    \n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print (\"Training a \",clf.__class__.__name__,\" using a training set size of \",len(X_train),\". . .\")\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    acc = predict_labels(clf, X_test, y_test)\n",
    "    print (acc)\n",
    "    print (\"F1 score and accuracy score for training set: \",\" , \",acc,\".\")\n",
    "    acc = predict_labels(clf, X_test, y_test)\n",
    "    print (\"F1 score and accuracy score for test set: \",\" , \",acc,\".\")\n",
    "#     pred(clf,data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a  LogisticRegression  using a training set size of  50000 . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in  3.9224276542663574  seconds\n",
      "Made predictions in  0.005010128021240234  seconds.\n",
      "0.093\n",
      "F1 score and accuracy score for training set:   ,  0.093 .\n",
      "Made predictions in  0.002005338668823242  seconds.\n",
      "F1 score and accuracy score for test set:   ,  0.093 .\n",
      "\n",
      "\n",
      "Training a  XGBClassifier  using a training set size of  50000 . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in  32.98190259933472  seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in  0.3859891891479492  seconds.\n",
      "0.1083\n",
      "F1 score and accuracy score for training set:   ,  0.1083 .\n",
      "Made predictions in  0.48729491233825684  seconds.\n",
      "F1 score and accuracy score for test set:   ,  0.1083 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "clf_A = LogisticRegression(random_state = 42)\n",
    "clf_B = SVC(random_state = 912, kernel='rbf')\n",
    "clf_C = xgb.XGBClassifier(seed = 2)\n",
    "\n",
    "train_predict(clf_A, traind, trainl, testd, testl)\n",
    "# pred(clf_A,pred_x.tail(1))\n",
    "print (\"\")\n",
    "# train_predict(clf_B, traind, trainl, testd, testl)\n",
    "# pred(clf_B,pred_x.tail(1))\n",
    "print (\"\")\n",
    "train_predict(clf_C, traind, trainl, testd, testl)\n",
    "# pred(clf_C,pred_x.tail(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
